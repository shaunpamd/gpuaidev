{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0b5326-74fd-4846-9f53-fbb5b51afdae",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama 3.2 Vision using Trainer on ROCm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3112a-1e22-4da0-9713-ded4483bb732",
   "metadata": {},
   "source": [
    "ðŸš¨ **WARNING**: This notebook is derived from [huggingface-llama-recipes](https://github.com/huggingface/huggingface-llama-recipes/blob/main/fine_tune/Llama-Vision%20FT.ipynb) .\n",
    "\n",
    "In this recipe, weâ€™ll demonstrate how to fine-tune a [Vision Language Model (VLM)](https://huggingface.co/blog/vlms) using the Hugging Face ecosystem.\n",
    "\n",
    "\n",
    "Transformers Trainer API makes it easy to fine-tune Llama-Vision models. One can also use parameter-efficient fine-tuning techniques out of the box thanks to transformers integration. Make sure to have latest version of transformers.\n",
    "\n",
    "\n",
    "We will fine-tune the model on a small split of VQAv2 dataset for educational purposes. If you want, you can also use a dataset where thereâ€™s multiple turns of conversation at one example. This dataset consists of images, questions about the images and short answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f58fe",
   "metadata": {},
   "source": [
    "# 1. Install Dependencies\n",
    "\n",
    "Letâ€™s start by installing the essential libraries weâ€™ll need for fine-tuning! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a605ea",
   "metadata": {},
   "source": [
    "We recommended to use official ROCm prebuilt Docker images with the framework pre-installed. Refer to [Rocm doc](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/pytorch-install.html#using-docker-with-pytorch-pre-installed)\n",
    "\n",
    "In the Docker container, check the availability of ROCm-capable accelerators using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8290175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a ROCm-GPU detected?  True\n",
      "How many ROCm-GPUs are detected?  8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is a ROCm-GPU detected? \", torch.cuda.is_available())\n",
    "print(\"How many ROCm-GPUs are detected? \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51257a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install `bitsandbytes` from source code for ROCm 6.0+.\n",
    "# Use -DBNB_ROCM_ARCH to target a specific GPU architecture.\n",
    "!git clone --recurse https://github.com/ROCm/bitsandbytes.git\n",
    "!cd bitsandbytes\n",
    "!git checkout rocm_enabled_multi_backend\n",
    "!pip install -r requirements-dev.txt\n",
    "!cmake -DBNB_ROCM_ARCH=\"gfx942\" -DCOMPUTE_BACKEND=hip -S .\n",
    "!python setup.py install\n",
    "\n",
    "# Install `bitsandbytes` from binary\n",
    "# Note, if you don't want to reinstall BNBs dependencies, append the `--no-deps` flag!\n",
    "!pip install --force-reinstall 'https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_multi-backend-refactor/bitsandbytes-0.44.1.dev0-py3-none-manylinux_2_24_x86_64.whl'\n",
    "\n",
    "# To leverage the SFTTrainer in TRL for model fine-tuning.\n",
    "!pip install trl\n",
    "\n",
    "# To leverage PEFT for efficiently adapting pre-trained language models .\n",
    "!pip install peft\n",
    "\n",
    "# Install the other dependencies.\n",
    "!pip install transformers datasets huggingface-hub scipy ipywidgets wandb accelerate\n",
    "\n",
    "# Tested with transformers==4.47.0, trl==0.12.0, datasets==3.1.0, bitsandbytes==0.44.1.dev0+9315692, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.19.1, accelerate==1.1.1, ipywidgets==8.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db325a",
   "metadata": {},
   "source": [
    "We must authenticate outselves before downloading the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb9635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e03968",
   "metadata": {},
   "source": [
    "# 2. Fine-Tune the Model using PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee883053-3f16-479e-9d55-49a05e62c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"merve/vqav2-small\", split=\"validation[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce170d7-4fc5-4b08-b4e3-b884b82c9495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['multiple_choice_answer', 'question', 'image'],\n",
       "    num_rows: 2144\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadba5ec-e4a6-480c-a09f-e4692fad6f79",
   "metadata": {},
   "source": [
    "We can now initialize the model and the processor, for we will use the processor in our preprocessing function. We will initialize the 11B variant of the vision model. \n",
    "\n",
    "Llama authors encourage freezing text decoder and only training image encoder. If you would like to try this out, feel free to set `FREEZE_LLM` to `True`. Intuitively, if your task is too domain specific, you might want to avoid this. In that case, you can either try LoRA training (which you can set `USE_LORA` to `True`), or freezing image encoder (set `FREEZE_IMAGE`Â to `True`) to save up compute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98466a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3209276cab404cffa79592206088e9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "trainable params: 31,416,320 || all params: 10,674,357,795 || trainable%: 0.2943\n"
     ]
    }
   ],
   "source": [
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "ckpt = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "USE_LORA = True\n",
    "FREEZE_LLM = False\n",
    "FREEZE_IMAGE = False\n",
    "\n",
    "if USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "        use_dora=True, # optional DoRA \n",
    "        init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            ckpt,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "elif FREEZE_IMAGE:\n",
    "    if FREEZE_LLM:\n",
    "        raise ValueError(\"You cannot freeze image encoder and text decoder at the same time.\")\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(ckpt,\n",
    "        torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    # freeze vision model to save up on compute\n",
    "    for param in model.vision_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "elif FREEZE_LLM:\n",
    "    if FREEZE_IMAGE:\n",
    "        raise ValueError(\"You cannot freeze image encoder and text decoder at the same time.\")\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(ckpt,\n",
    "        torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    # freeze text model, this is encouraged in paper\n",
    "    for param in model.language_model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "else: # full ft\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(ckpt,\n",
    "        torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72738779-b3b9-424b-92c1-c72b59898543",
   "metadata": {},
   "source": [
    "For preprocessing, we will put together questions and answers. In between questions and answers we will put a conditioning phrase, which will condition the model and trigger question answering, in this case itâ€™s â€œAnswer briefly.â€. \n",
    "To process images, we simply have to batch every image and put them as list of singular images. This is needed due to how processor can take a list of multiple images at once with a single text input, so we have to indicate that these are single images for each example.\n",
    "Lastly, we will set pad tokens and image tokens to -100 to make model ignore these tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467eeb17-092a-42fd-9ad7-48642cc03731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(examples):\n",
    "    texts = [f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>{example['question']} Answer briefly. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{example['multiple_choice_answer']}<|eot_id|>\" for example in examples]\n",
    "    images = [[example[\"image\"].convert(\"RGB\")] for example in examples]\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100 \n",
    "    labels[labels == 128256] = -100 # image token index\n",
    "    batch[\"labels\"] = labels\n",
    "    batch = batch.to(torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e0923-d0ef-43b7-9db5-5008f78b2782",
   "metadata": {},
   "source": [
    "We can now setup our Trainer. Before that, we will setup the arguments we pass to the \n",
    "Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19d7bab-aa59-4c78-81a1-10724efe97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "args=TrainingArguments(\n",
    "            num_train_epochs=2,\n",
    "            remove_unused_columns=False,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=1e-6,\n",
    "            adam_beta2=0.999,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",\n",
    "            optim=\"adamw_hf\",\n",
    "            push_to_hub=False,\n",
    "            save_total_limit=1,\n",
    "            bf16=True,\n",
    "            output_dir=\"./lora\",\n",
    "            dataloader_pin_memory=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083279f-b084-4e6c-8175-4986cc53ad2a",
   "metadata": {},
   "source": [
    "We can now initialize the Trainer and start training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2971d6-318e-4d4a-952e-6a15d511ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=ds,\n",
    "        data_collator=process,\n",
    "        args=args\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d91a6",
   "metadata": {},
   "source": [
    "Call train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fca52-c078-481f-922e-c22c49ce412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myahao-he\u001b[0m (\u001b[33myahao-he-Tsinghua University\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/billhe/rocm-finetune-example/wandb/run-20250106_152522-ma9wrmcg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yahao-he-Tsinghua%20University/huggingface/runs/ma9wrmcg' target=\"_blank\">./lora</a></strong> to <a href='https://wandb.ai/yahao-he-Tsinghua%20University/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yahao-he-Tsinghua%20University/huggingface' target=\"_blank\">https://wandb.ai/yahao-he-Tsinghua%20University/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yahao-he-Tsinghua%20University/huggingface/runs/ma9wrmcg' target=\"_blank\">https://wandb.ai/yahao-he-Tsinghua%20University/huggingface/runs/ma9wrmcg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='1072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/1072 03:19 < 1:09:06, 0.25 it/s, Epoch 0.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
