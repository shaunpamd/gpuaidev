{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Finetune Llama-3.1 8B with torchtune\n",
    "\n",
    "This tutorial demonstrates how to fine-tune the Llama-3.1 8B large language model (LLM) on AMD ROCm GPUs by leveraging torchtune. Torchtune is a PyTorch library for easily authoring, post-training, and experimenting with LLMs. It provides: 1) Hackable training recipes for SFT, knowledge distillation, RL and RLHF, and quantization-aware training. 2) Simple PyTorch implementations of popular LLMs like Llama, Gemma, Mistral, Phi, Qwen, and more. 3) OOTB best-in-class memory efficiency, performance improvements, and scaling, utilizing the latest PyTorch APIs. 4) YAML configs for easily configuring training, evaluation, quantization or inference recipes. For more information, see [Torchtune github official page](https://github.com/pytorch/torchtune)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinctâ„¢ GPUs**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.x**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    rocm-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPU(s) with relevant details, similar to the image below.\n",
    "\n",
    "   ![rocm-smi](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access [Meta's Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.2-3B).\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "* This tutorial uses a sample dataset from Hugging Face, which is prepared during the setup steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "## Prepare the training environment\n",
    "\n",
    "### 1. Pull the Docker image\n",
    "\n",
    "Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/pytorch-training:latest\n",
    "```\n",
    "\n",
    "### 2. Launch the Docker container\n",
    "\n",
    "Launch the Docker container and map the necessary directories. Replace `/path/to/notebooks` with the full path to the directory on your host machine where these notebooks are stored.\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  --hostname=ROCm-FT \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch-training:latest\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "### 3. Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3698c",
   "metadata": {},
   "source": [
    "### 4. Install the required libraries\n",
    "\n",
    "Install the libraries required for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a51315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/rocm6.3/\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.7.0.dev20250302+rocm6.3)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/nightly/rocm6.3/torchvision-0.22.0.dev20250302%2Brocm6.3-cp312-cp312-linux_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting torchao\n",
      "  Downloading https://download.pytorch.org/whl/nightly/rocm6.3/torchao-0.10.0.dev20250303%2Brocm6.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.2.0+git4b3bb1f8 in /opt/conda/lib/python3.12/site-packages (from torch) (3.2.0+git4b3bb1f8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/nightly/rocm6.3/torchvision-0.22.0.dev20250302%2Brocm6.3-cp312-cp312-linux_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/rocm6.3/torchao-0.10.0.dev20250303%2Brocm6.3-py3-none-any.whl (687 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m687.6/687.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, torchvision\n",
      "Successfully installed torchao-0.10.0.dev20250303+rocm6.3 torchvision-0.22.0.dev20250302+rocm6.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/rocm6.3/\n",
      "Requirement already satisfied: torchtune in /opt/conda/lib/python3.12/site-packages (0.6.0.dev20250302+rocm6.3)\n",
      "Requirement already satisfied: torchdata in /opt/conda/lib/python3.12/site-packages (from torchtune) (0.11.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (from torchtune) (3.3.2)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /opt/conda/lib/python3.12/site-packages (from torchtune) (0.29.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.12/site-packages (from torchtune) (0.5.3)\n",
      "Requirement already satisfied: kagglehub in /opt/conda/lib/python3.12/site-packages (from torchtune) (0.3.10)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (from torchtune) (0.9.0)\n",
      "Requirement already satisfied: blobfile>=2 in /opt/conda/lib/python3.12/site-packages (from torchtune) (3.0.0)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.12/site-packages (from torchtune) (0.21.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchtune) (2.2.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torchtune) (4.66.5)\n",
      "Requirement already satisfied: omegaconf in /opt/conda/lib/python3.12/site-packages (from torchtune) (2.4.0.dev3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from torchtune) (7.0.0)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /opt/conda/lib/python3.12/site-packages (from torchtune) (11.1.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /opt/conda/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /opt/conda/lib/python3.12/site-packages (from blobfile>=2->torchtune) (2.2.3)\n",
      "Requirement already satisfied: lxml>=4.9 in /opt/conda/lib/python3.12/site-packages (from blobfile>=2->torchtune) (5.3.1)\n",
      "Requirement already satisfied: filelock>=3.0 in /opt/conda/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (3.11.13)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/lib/python3.12/site-packages (from torchdata->torchtune) (2.7.0.dev20250302+rocm6.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2->torchdata->torchtune) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2->torchdata->torchtune) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2->torchdata->torchtune) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2->torchdata->torchtune) (3.1.4)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.2.0+git4b3bb1f8 in /opt/conda/lib/python3.12/site-packages (from torch>=2->torchdata->torchtune) (3.2.0+git4b3bb1f8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.3->torch>=2->torchdata->torchtune) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2->torchdata->torchtune) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install PyTorch, torchvision, torchao nightlies\n",
    "!pip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/rocm6.3/\n",
    "!pip install --pre --upgrade torchtune --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.3/ \n",
    "# This note book is verified under torch==2.7.0.dev20250226+rocm6.3, torchao==0.10.0.dev20250227+rocm6.3, torchtune==0.6.0.dev20250226+rocm6.3, torchvision==0.22.0a0+06a925c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f703213",
   "metadata": {},
   "source": [
    "Verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cfc30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-triton-rocm     3.2.0+git4b3bb1f8\n",
      "torch                   2.7.0.dev20250302+rocm6.3\n",
      "torchdata               0.11.0\n",
      "torchtitan              0.0.2\n",
      "torchtune               0.6.0.dev20250302+rocm6.3\n"
     ]
    }
   ],
   "source": [
    "# Verify the installation and version of the required libraries\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c923c",
   "metadata": {},
   "source": [
    "Here is the expected output:\n",
    "\n",
    "```\n",
    "pytorch-triton-rocm     3.2.0+git4b3bb1f8\n",
    "torch                   2.7.0.dev20250302+rocm6.3\n",
    "torchdata               0.11.0\n",
    "torchtune               0.6.0.dev20250302+rocm6.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea665d8",
   "metadata": {},
   "source": [
    "### 5. Verify torchtune is ready for ROCm 6.3\n",
    "\n",
    "To confirm that the package is installed correctly, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1775d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tune [-h] {download,ls,cp,run,validate,cat} ...\n",
      "\n",
      "Welcome to the torchtune CLI!\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "subcommands:\n",
      "  {download,ls,cp,run,validate,cat}\n",
      "    download            Download a model from the Hugging Face Hub or Kaggle\n",
      "                        Model Hub.\n",
      "    ls                  List all built-in recipes and configs\n",
      "    cp                  Copy a built-in recipe or config to a local path.\n",
      "    run                 Run a recipe. For distributed recipes, this supports\n",
      "                        all torchrun arguments.\n",
      "    validate            Validate a config and ensure that it is well-formed.\n",
      "    cat                 Pretty print a config, making it easy to know which\n",
      "                        parameters you can override with `tune run`.\n"
     ]
    }
   ],
   "source": [
    "!tune --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0aa46a",
   "metadata": {},
   "source": [
    "And should see the following output:\n",
    "\n",
    "```\n",
    "usage: tune [-h] {download,ls,cp,run,validate,cat} ...\n",
    "\n",
    "Welcome to the torchtune CLI!\n",
    "\n",
    "options:\n",
    "  -h, --help            show this help message and exit\n",
    "\n",
    "subcommands:\n",
    "  {download,ls,cp,run,validate,cat}\n",
    "    download            Download a model from the Hugging Face Hub or Kaggle\n",
    "                        Model Hub.\n",
    "    ls                  List all built-in recipes and configs\n",
    "    cp                  Copy a built-in recipe or config to a local path.\n",
    "    run                 Run a recipe. For distributed recipes, this supports\n",
    "                        all torchrun arguments.\n",
    "    validate            Validate a config and ensure that it is well-formed.\n",
    "    cat                 Pretty print a config, making it easy to know which\n",
    "                        parameters you can override with `tune run`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d0c48",
   "metadata": {},
   "source": [
    "\n",
    "**âš ï¸ Important: ensure the correct kernel is selected**\n",
    "\n",
    "If the verification process fails, ensure the correct Jupyter kernel is selected for your notebook.\n",
    "To change the kernel, follow these steps:\n",
    "\n",
    "1. Go to the **Kernel** menu.\n",
    "2. Select **Change Kernel**\n",
    "3. Select `Python 3 (ipykernel)` from the list.\n",
    "\n",
    "**Failure to select the correct kernel can lead to unexpected issues when running the notebook.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad2378",
   "metadata": {},
   "source": [
    "### 6. Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-3.1. Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.1 8B ](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct). Tokens typically start with \"hf_\". \n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "**Note**: Uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53e64b",
   "metadata": {},
   "source": [
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb55cf-7f2d-45c6-9c5c-86a82ca4c9c6",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "This section covers the process of setting up and running fine-tuning for the Llama-3.1 model using torchtune. The following steps describe how to set up GPUs, import the required libraries, configure the model and training parameters, and run the fine-tuning process.\n",
    "\n",
    "**âš ï¸ Important: ensure the correct kernel is selected**\n",
    "\n",
    "Ensure the correct Jupyter kernel is selected for your notebook. To change the kernel, follow these steps:\n",
    "\n",
    "1. Go to the **Kernel** menu.\n",
    "2. Select **Change Kernel**\n",
    "3. Select `Python 3 (ipykernel)` from the list.\n",
    "\n",
    "**Failure to select the correct kernel can lead to unexpected issues when running the notebook.**\n",
    "\n",
    "### Set and verify the GPU availability\n",
    "\n",
    "Begin by specifying the GPUs available for fine-tuning. Verify that they are properly detected by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fb94e7-c059-4883-97dc-c36546e65236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch detected number of available devices: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "gpus= [0, 1] # Rank 0 is for MI300x single device finetune, and Rank 0/1 for full \n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "# Ensure PyTorch detects the GPUs correctly\n",
    "print(f\"PyTorch detected number of available devices: {torch.cuda.device_count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3982-f46e-47e5-b4f7-f9fbf873a2fc",
   "metadata": {},
   "source": [
    "### Downloading Llama model\n",
    "\n",
    "You can run the following command to download the weights to your local machine. This will also download the tokenizer model and a responsible use guide.\n",
    "\n",
    "To download Llama3.1, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26886732-b369-495f-8b6b-decdf0564219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: original/consolidated.00.pth\n",
      ".gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.52k/1.52k [00:00<00:00, 17.1MB/s]\n",
      "LICENSE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.63k/7.63k [00:00<00:00, 27.9MB/s]\n",
      "README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.0k/44.0k [00:00<00:00, 2.09MB/s]\n",
      "USE_POLICY.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.69k/4.69k [00:00<00:00, 19.8MB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 855/855 [00:00<00:00, 5.76MB/s]\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:00<00:00, 1.29MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.98G/4.98G [00:07<00:00, 655MB/s]\n",
      "model-00002-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5.00G/5.00G [00:06<00:00, 807MB/s]\n",
      "model-00003-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4.92G/4.92G [00:06<00:00, 736MB/s]\n",
      "model-00004-of-00004.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.17G/1.17G [00:01<00:00, 591MB/s]\n",
      "model.safetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.9k/23.9k [00:00<00:00, 4.20MB/s]\n",
      "original%2Fparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 1.32MB/s]\n",
      "tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.18M/2.18M [00:00<00:00, 198MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296/296 [00:00<00:00, 1.97MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.09M/9.09M [00:00<00:00, 15.3MB/s]\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55.4k/55.4k [00:00<00:00, 130MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/.cache\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/.gitattributes\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/LICENSE\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/README.md\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/USE_POLICY.md\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/config.json\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/generation_config.json\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/model-00001-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/model-00002-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/model-00003-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/model-00004-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/original\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/special_tokens_map.json\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/tokenizer.json\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/tokenizer_config.json\n",
      "/tmp/Meta-Llama-3.1-8B-Instruct/original_repo_id.json\n"
     ]
    }
   ],
   "source": [
    "!tune download meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --output-dir /tmp/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --ignore-patterns \"original/consolidated.00.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d747b-4ef6-4969-9d15-f9834a5ee6bb",
   "metadata": {},
   "source": [
    "### Running finetuning recipes\n",
    "\n",
    "#### Single GPU traning\n",
    "You can finetune Llama3.1 8B with LoRA on a single GPU using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c916dcd-fc94-4214-895a-9720ad3ec3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
      "  packed: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tmp/torchtune/llama3_1_8B/lora_single_device/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.lora_llama3_1_8b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: false\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model\n",
      "\n",
      "Setting manual seed to local seed 1770672408. Local seed is seed + rank = 1770672408 + 0\n",
      "Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "Writing logs to /tmp/torchtune/llama3_1_8B/lora_single_device/logs/log_1740986149.txt\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory active: 15.06 GiB\n",
      "\tGPU peak memory alloc: 15.06 GiB\n",
      "\tGPU peak memory reserved: 15.18 GiB\n",
      "Tokenizer is initialized from file.\n",
      "Optimizer and loss are initialized.\n",
      "Loss is initialized.\n",
      "Learning rate scheduler is initialized.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "1|3|Loss: 1.8343863487243652:   0%|          | 3/3235 [00:09<2:24:40,  2.69s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 52, in main\n",
      "    parser.run(args)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 46, in run\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 214, in _run_cmd\n",
      "    self._run_single_device(args, is_builtin=is_builtin)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 108, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 286, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 803, in <module>\n",
      "    sys.exit(recipe_main())\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 798, in recipe_main\n",
      "    recipe.train()\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 699, in train\n",
      "    current_loss = self._loss_step(batch) * current_num_tokens\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 640, in _loss_step\n",
      "    logits = self._model(**batch)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/modules/transformer.py\", line 648, in forward\n",
      "    h = layer(\n",
      "        ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py\", line 171, in forward\n",
      "    return self.checkpoint_fn(  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/_compile.py\", line 51, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 765, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py\", line 495, in checkpoint\n",
      "    ret = function(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/modules/transformer.py\", line 122, in forward\n",
      "    attn_out = self.attn(h, h, mask=mask, input_pos=input_pos)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self.^C\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe90d81-e99e-46fd-bf96-5246210f75df",
   "metadata": {},
   "source": [
    "#### Distributed training\n",
    "For distributed training, tune CLI integrates with torchrun. Note that we must have more than 1 GPU in hand to run distributed training example. To run a full finetune of Llama3.1 8B on two GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235355d2-634d-4444-8497-71058a1e473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n",
      "W0303 07:14:28.922000 95070 site-packages/torch/distributed/run.py:766] \n",
      "W0303 07:14:28.922000 95070 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0303 07:14:28.922000 95070 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0303 07:14:28.922000 95070 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "Running FullFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/full\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  packed: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tmp/torchtune/llama3_1_8B/full/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.llama3_1_8b\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 2.0e-05\n",
      "optimizer_in_bwd: false\n",
      "output_dir: /tmp/torchtune/llama3_1_8B/full\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/full/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model\n",
      "\n",
      "Running FullFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/full\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  packed: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tmp/torchtune/llama3_1_8B/full/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.llama3_1_8b\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 2.0e-05\n",
      "optimizer_in_bwd: false\n",
      "output_dir: /tmp/torchtune/llama3_1_8B/full\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/full/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model\n",
      "\n",
      "Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma7\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma6\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma5\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma4\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma3\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma2\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/xeth0\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma1\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0\n",
      "libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 6 (supports 1 to 1) for device /sys/class/infiniband/rdma0\n",
      "Setting manual seed to local seed 3009749264. Local seed is seed + rank = 3009749264 + 0\n",
      "Writing logs to /tmp/torchtune/llama3_1_8B/full/logs/log_1740986077.txt\n",
      "Distributed training is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "Instantiating model and loading checkpoint took 5.75 secs\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory active: 8.47 GiB\n",
      "\tGPU peak memory alloc: 8.47 GiB\n",
      "\tGPU peak memory reserved: 8.62 GiB\n",
      "Optimizer is initialized.\n",
      "Loss is initialized.\n",
      "README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.47k/7.47k [00:00<00:00, 27.0MB/s]\n",
      "(â€¦)-00000-of-00001-a09b74b3ef9c3b56.parquet: 100%|â–ˆ| 24.2M/24.2M [00:01<00:00, 2\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 52002/52002 [00:00<00:00, 381740.82 examples/s]\n",
      "Using the latest cached version of the dataset since tatsu-lab/alpaca couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/tatsu-lab___alpaca/default/0.0.0/dce01c9b08f87459cf36a430d809084718273017 (last modified on Mon Mar  3 07:14:49 2025).\n",
      "No learning rate scheduler configured. Using constant learning rate.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "1|11|Loss: 1.1556004285812378:   0%|       | 11/13000 [00:13<3:02:15,  1.19it/s]W0303 07:15:34.965000 95070 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\n",
      "W0303 07:15:34.966000 95070 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 95376 closing signal SIGINT\n",
      "W0303 07:15:34.966000 95070 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 95377 closing signal SIGINT\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/recipes/full_finetune_distributed.py\", line 953, in <module>\n",
      "[rank1]:     sys.exit(recipe_main())\n",
      "[rank1]:              ^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "[rank1]:     sys.exit(recipe_main(conf))\n",
      "[rank1]:              ^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/recipes/full_finetune_distributed.py\", line 948, in recipe_main\n",
      "[rank1]:     recipe.train()\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/recipes/full_finetune_distributed.py\", line 828, in train\n",
      "[rank1]:     training.scale_grads(self._model, self.world_size / num_tokens)\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/torchtune/training/_grad_scaler.py\", line 31, in scale_grads\n",
      "[rank1]:     p.grad *= scaler\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/torch/_compile.py\", line 51, in inner\n",
      "[rank1]:     return disable_fn(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 765, in _fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/torch/distributed/tensor/_api.py\", line 344, in __torch_dispatch__\n",
      "[rank1]:     return DTensor._op_dispatcher.dispatch(\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py\", line 219, in dispatch\n",
      "[rank1]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)\n",
      "[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/opt/conda/lib/python3.12/site-packages/torch/_ops.py\", line 756, in __call__\n",
      "[rank1]:     return self._op(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]: KeyboardInterrupt\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/recipes/full_finetune_distributed.py\", line 953, in <module>\n",
      "[rank0]:     sys.exit(recipe_main())\n",
      "[rank0]:              ^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "[rank0]:     sys.exit(recipe_main(conf))\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/recipes/full_finetune_distributed.py\", line 948, in recipe_main\n",
      "[rank0]:     recipe.train()\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/recipes/full_finetune_distributed.py\", line 828, in train\n",
      "[rank0]:     training.scale_grads(self._model, self.world_size / num_tokens)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torchtune/training/_grad_scaler.py\", line 31, in scale_grads\n",
      "[rank0]:     p.grad *= scaler\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torch/_compile.py\", line 51, in inner\n",
      "[rank0]:     return disable_fn(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 765, in _fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torch/distributed/tensor/_api.py\", line 344, in __torch_dispatch__\n",
      "[rank0]:     return DTensor._op_dispatcher.dispatch(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torch/distributed/tensor/_dispatch.py^C\n",
      "W0303 07:15:35.015000 95070 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 95376 closing signal SIGTERM\n",
      "W0303 07:15:35.016000 95070 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 95377 closing signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "!tune run --nproc_per_node 2 full_finetune_distributed --config llama3_1/8B_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce556ea-9525-46ab-bcf2-1fe2e8c319d2",
   "metadata": {},
   "source": [
    "### Customize your recipes for Llama\n",
    "\n",
    "There are two ways in which you can modify configs:\n",
    "\n",
    "#### Config Overrides\n",
    "\n",
    "You can directly overwrite config fields from the command line, e.g. We can set batch size to 16 and disable activation checkpoint in the meantime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d525b641-c645-4987-9ae6-173d6a75e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 16\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
      "  packed: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: false\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tmp/torchtune/llama3_1_8B/lora_single_device/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.lora_llama3_1_8b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: false\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model\n",
      "\n",
      "Setting manual seed to local seed 319935670. Local seed is seed + rank = 319935670 + 0\n",
      "Writing logs to /tmp/torchtune/llama3_1_8B/lora_single_device/logs/log_1740986459.txt\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory active: 15.06 GiB\n",
      "\tGPU peak memory alloc: 15.06 GiB\n",
      "\tGPU peak memory reserved: 15.18 GiB\n",
      "Tokenizer is initialized from file.\n",
      "Optimizer and loss are initialized.\n",
      "Loss is initialized.\n",
      "Learning rate scheduler is initialized.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "1|2|Loss: 1.6996852159500122:   0%|             | 2/404 [00:15<49:23,  7.37s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 52, in main\n",
      "    parser.run(args)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 46, in run\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 214, in _run_cmd\n",
      "    self._run_single_device(args, is_builtin=is_builtin)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 108, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 286, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 803, in <module>\n",
      "    sys.exit(recipe_main())\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 798, in recipe_main\n",
      "    recipe.train()\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 688, in train\n",
      "    utils.batch_to_device(batch, self._device)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/utils/_device.py\", line 190, in batch_to_device\n",
      "    batch[k] = v.to(device)\n",
      "               ^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device \\\n",
    "    --config llama3_1/8B_lora_single_device \\\n",
    "    batch_size=16 \\\n",
    "    enable_activation_checkpointing=False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f6b7-4a2f-467c-b1fd-e53bf7dcd837",
   "metadata": {},
   "source": [
    "**Note**ï¼šIf you encounter out-of-memory (OOM) errors, reduce the `batch_size` or enable gradient checkpointing. Use rocm-smi to monitor VRAM usage during fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d33c-95b2-467c-8212-96a7810b5e3c",
   "metadata": {},
   "source": [
    "#### Update a Local Copy\n",
    "\n",
    "You can also copy the config to your local directory and modify the contents directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3eecd4-a968-42dd-89e2-f94b4633ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to my_custom_config_llama3_1_8B_lora_single_device.yaml\n"
     ]
    }
   ],
   "source": [
    "!tune cp llama3_1/8B_lora_single_device ./my_custom_config_llama3_1_8B_lora_single_device.yaml\n",
    "# Copied to ./my_custom_config_llama3_1_8B_lora_single_device.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9baef-c9a4-4438-aa88-cbcdcec199b9",
   "metadata": {},
   "source": [
    "Then, you can run your custom recipe by directing the tune run command to your local files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "444a176a-a893-46ee-8df9-b9bc4ddca1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
      "  packed: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tmp/torchtune/llama3_1_8B/lora_single_device/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.lora_llama3_1_8b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: false\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 8\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/torchtune/llama3_1_8B/lora_single_device/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model\n",
      "\n",
      "Setting manual seed to local seed 3993579082. Local seed is seed + rank = 3993579082 + 0\n",
      "Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "Writing logs to /tmp/torchtune/llama3_1_8B/lora_single_device/logs/log_1740986560.txt\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory active: 15.06 GiB\n",
      "\tGPU peak memory alloc: 15.06 GiB\n",
      "\tGPU peak memory reserved: 15.18 GiB\n",
      "Tokenizer is initialized from file.\n",
      "Optimizer and loss are initialized.\n",
      "Loss is initialized.\n",
      "Learning rate scheduler is initialized.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "1|48|Loss: 0.9399025440216064:   1%|        | 48/3235 [01:26<1:28:27,  1.67s/it]Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 52, in main\n",
      "    parser.run(args)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 46, in run\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 214, in _run_cmd\n",
      "    self._run_single_device(args, is_builtin=is_builtin)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 108, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 286, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 803, in <module>\n",
      "    sys.exit(recipe_main())\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 798, in recipe_main\n",
      "    recipe.train()\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 699, in train\n",
      "    current_loss = self._loss_step(batch) * current_num_tokens\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/recipes/lora_finetune_single_device.py\", line 640, in _loss_step\n",
      "    logits = self._model(**batch)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/modules/transformer.py\", line 648, in forward\n",
      "    h = layer(\n",
      "        ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py\", line 171, in forward\n",
      "    return self.checkpoint_fn(  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/_compile.py\", line 51, in inner\n",
      "    return disable_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 765, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py\", line 495, in checkpoint\n",
      "    ret = function(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torchtune/modules/transformer.py\", line 122, in forward\n",
      "    attn_out = self.attn(h, h, mask=mask, input_pos=input_pos)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self.^C\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device --config ./my_custom_config_llama3_1_8B_lora_single_device.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52620657",
   "metadata": {},
   "source": [
    "Check out tune --help for all possible CLI commands and options. For more information on using and updating configs, please refer to torchtune official [deep-dive](https://pytorch.org/torchtune/main/deep_dives/configs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38005e08-f1e8-4cde-a139-b18e74e41bb8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Custom Datasets\n",
    "\n",
    "`torchtune` supports finetuning on a variety of different datasets, including instruct-style, chat-style, preference datasets, and more. If you want to learn more about how to apply these components to finetune on your own custom dataset, please check out the provided links along with torchtune API [docs](https://pytorch.org/torchtune/main/api_ref_datasets.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3834-0c06-4907-83cf-8f8ff9348f88",
   "metadata": {},
   "source": [
    "### Monitoring GPU memory\n",
    "\n",
    "To monitor GPU memory during training, run the following command in a terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91846614",
   "metadata": {},
   "source": [
    "This command displays memory usage and other GPU metrics to ensure your hardware resources are being optimally used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
