{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDIM pretraining \n",
    "\n",
    "This tutorial walks you through how to pretrain a **Denoising Diffusion Implicit Model (DDIM)** using the [Hugging Face Diffusers library](https://github.com/huggingface/diffusers) on AMD GPUs. You'll train a U-Net-based DDIM model to generate realistic flower images from the Flowers-102 dataset.\n",
    "\n",
    "**Note**: This tutorial was prepared by [Cătălin (Constantin) Milu](https://1y33.github.io/).\n",
    "\n",
    "## Model and dataset overview\n",
    "\n",
    "This tutorial uses the Flowers-102 dataset, which contains images of flowers from 102 different categories. The dataset provides a wide variety of textures, colors, and shapes, making it ideal for training a diffusion model. During pretraining, the model learns to generate diverse flower images. Fine-tuning then adapts the model to generate higher-quality images and style-specific outputs.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup:\n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct™ GPUs**:  This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you use an AMD Instinct GPU with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.2.4**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html).\n",
    "* **Python 3.7+**\n",
    "\n",
    "## Prepare the inference environment\n",
    "\n",
    "This section first creates a virtual environment and then starts the Jupyter server. \n",
    "\n",
    "### Install the dependencies\n",
    "\n",
    "Start by creating a virtual environment:\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate \n",
    "```\n",
    "\n",
    "### Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then start the Jupyter server:\n",
    "\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "\n",
    "Install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision  --index-url https://download.pytorch.org/whl/rocm6.2\n",
    "!pip install matplotlib transformers diffusers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Torch installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Is ROCm available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Config class\n",
    "\n",
    "Next, define the `Config` class to store the training parameters. This lets you easily reuse and modify the configuration for future runs. Adjusting these parameters affects training times and the quality of the generated images, so experiment with different values to find the optimal setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    image_size = 128                                # Size of the training images\n",
    "    train_batch_size = 16                           # Batch size for training\n",
    "    eval_batch_size = 16                            # Batch size for evaluation\n",
    "    num_epochs = 100                                # Total number of training epochs\n",
    "    learning_rate = 1e-4                            # Learning rate for optimization\n",
    "    lr_warmup_steps = 500                           # Warmup steps for learning rate scheduling\n",
    "    save_image_epochs = 10                          # Frequency of saving generated images\n",
    "    save_model_epochs = 30                          # Frequency of saving model checkpoints\n",
    "    output_dir = \"ddim-flowers-128\"                 # Output directory for model and images\n",
    "    seed = 36                                       # Random seed for reproducibility\n",
    "    dataset_name = \"huggan/flowers-102-categories\"  # Name of the dataset\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the training dataset\n",
    "\n",
    "Load the dataset and display a few images to confirm that everything is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "\n",
    "# Visualize some images\n",
    "fig, axs = plt.subplots(1, 5, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[\"image\"]):\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].axis(\"off\")  \n",
    "    if (i + 1) % 5 == 0:  # Show only 5 images\n",
    "        break\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the transforms for the dataset\n",
    "\n",
    "Before training, apply transformations to the images to ensure they have the correct size and format. This involves resizing the images, normalizing their pixel values, and applying data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),  # Normalize to [-1, 1] range\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    images = [transformations(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model \n",
    "\n",
    "To create the model, use a [U-Net](https://arxiv.org/abs/1505.04597) architecture, implemented using the `UNet2DModel` from the `diffusers` library. U-Net is a widely used architecture for denoising diffusion models due to its encoder-decoder structure with skip connections, which help preserve spatial information while allowing deep feature extraction. This makes it highly effective for generating high-quality images in diffusion-based models.\n",
    "\n",
    "`AttnDownBlock2D` and `AttnUpBlock2D` improve the model’s ability to capture long-range dependencies, which standard convolutions struggle with. By letting each pixel attend to relevant regions across the image, attention enhances feature refinement and structure preservation, leading to more coherent and detailed image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size, \n",
    "    in_channels=3,  \n",
    "    out_channels=3,  \n",
    "    layers_per_block=2, \n",
    "    dropout=0.1, \n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # Channels per block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  \n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  \n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  \n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  \n",
    "        \"AttnUpBlock2D\", \n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\", \n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ").to(config.device)\n",
    "\n",
    "# Printing the model summary \n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "total_size_mb = total_params * 4 / (1024 ** 2)\n",
    "\n",
    "print(\"Total Model Parameters:\", f\"{total_params:,}\")\n",
    "print(\"Total Model Size: {:.2f} MB\".format(total_size_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined, you can now simulate the diffusion process using a noise scheduler. This adds noise to an image in a step-by-step manner, which the model later learns to reverse during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "# Select an image from the dataset\n",
    "image = dataset[0][\"images\"].unsqueeze(0)\n",
    "\n",
    "# Define the noise scheduler\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn_like(image)\n",
    "timesteps = torch.tensor([100], dtype=torch.long)\n",
    "\n",
    "# Add noise to the image\n",
    "noisy_image = noise_scheduler.add_noise(image, noise, timesteps)\n",
    "\n",
    "# Convert to PIL image for visualization\n",
    "Image.fromarray(\n",
    "    ((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).clamp(0, 255).byte().numpy()[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the optimizer and learning rate scheduler\n",
    "\n",
    "Use the Adam optimizer, which is one of the most widely used optimizers in deep learning due to its adaptive learning rate and efficient performance across various tasks.\n",
    "\n",
    "For learning rate scheduling, apply a \"cosine decay schedule\" with warmup, as implemented in the `diffusers` library. This approach gradually increases the learning rate at the start of training (the warmup phase) to stabilize updates, then decays it smoothly following a cosine curve. This helps prevent sudden drops in performance and lets the model converge more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "This function generates images using the trained model and saves them as a grid for visual inspection. Use the DDIMPipeline from the `diffusers` library to create the images and save the output in the `samples` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Generate images with the trained pipeline\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(config.seed)\n",
    "    ).images\n",
    "    \n",
    "    # Create a grid from the generated images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "    \n",
    "    # Define the directory to save the generated images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the image grid\n",
    "    image_grid.save(f\"{test_dir}/{epoch}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The following code defines the training loop for the DDIM model. It ensures stable and efficient training by iterating over the dataset and updating the parameters for the model at each epoch. At regular intervals, it performs evaluations and saves model checkpoints to monitor the progress and improve performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from huggingface_hub import HfApi, Repository, create_repo \n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Ensure output directory is set up\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    global_step = 0\n",
    "    # Training loop\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        # Iterate through the batches in the dataloader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[\"images\"].to(config.device)\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Generate random timesteps for the noise scheduler\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (bs,),\n",
    "                device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            # Forward pass to predict the noise\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "                \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()        # Update model weights\n",
    "            lr_scheduler.step()     # Update the learning rate\n",
    "            optimizer.zero_grad()   # Reset gradients\n",
    "\n",
    "            global_step += 1\n",
    "            epoch_loss += loss.detach().item()\n",
    "            \n",
    "            # Logging and progress tracking\n",
    "            logs = {\n",
    "                \"loss\": loss.detach().item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                \"step\": global_step,\n",
    "            }\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "        pipeline = DDIMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "        \n",
    "        # Save images at regular intervals\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            evaluate(config, epoch, pipeline)\n",
    "        \n",
    "        # Save the model at regular intervals\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            pipeline.save_pretrained(config.output_dir)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the training\n",
    "\n",
    "Use the following command to launch the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the generated images\n",
    "\n",
    "This section demonstrates how to visualize the images generated during the training process. This helps you inspect the quality and diversity of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
    "Image.open(sample_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "[https://huggingface.co/docs/diffusers/en/tutorials/basic_training](https://huggingface.co/docs/diffusers/en/tutorials/basic_training)\n",
    "\n",
    " This tutorial provides a comprehensive guide on how to train models using the `diffusers` library, from setting up the dataset to optimizing and running training loops.\n",
    "\n",
    "---\n",
    "\n",
    "[https://huggingface.co/docs/diffusers/en/tutorials/autopipeline](https://huggingface.co/docs/diffusers/en/tutorials/autopipeline)\n",
    "\n",
    " Learn how to leverage the Hugging Face AutoPipeline feature, which simplifies the training pipeline by automatically managing different components.\n",
    "\n",
    "---\n",
    "\n",
    "[https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline](https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline)\n",
    "\n",
    " A step-by-step guide on how to build your own custom pipeline using the `diffusers` library, offering flexibility to adapt models to specific use cases.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
